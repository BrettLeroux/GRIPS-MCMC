{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.distributions import Normal, Bernoulli\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGPI = np.log(np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarmonicTrialFunction(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(HarmonicTrialFunction, self).__init__()\n",
    "        self.alpha = nn.Parameter(alpha)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # outputs logprob\n",
    "        # 2.0 * because it's |\\Psi|^2\n",
    "        return 2.0 * ( 0.5*torch.log(self.alpha) - 0.25*LOGPI - 0.5*x*x*self.alpha*self.alpha )\n",
    "\n",
    "    def local_energy(self, x):\n",
    "        return self.alpha*self.alpha + (x*x) * (1.0 - self.alpha**4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log |\\Psi(x)|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_mean_energy(alpha):\n",
    "    return ((alpha**2)/2) + (1.0/(2*(alpha**2)))\n",
    "def true_variance(alpha):\n",
    "    return ((alpha**4 - 1)**2)/(2*alpha**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_proposal(old_point):\n",
    "    # symmetric\n",
    "    return Normal(old_point, 0.3*torch.ones_like(old_point)).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_mh_acceptance_ratio(logprob, old_config, new_config):\n",
    "    logacc = torch.min(torch.tensor(0.0), logprob(new_config) - logprob(old_config))\n",
    "    return torch.exp(logacc)\n",
    "\n",
    "def asymmetric_mh_acceptance_ratio(logprob, new_old_logprob, old_new_logprob, old_config, new_config):\n",
    "    logacc = torch.min(torch.tensor(0.0), logprob(new_config) + \n",
    "                       old_new_logprob\n",
    "                       - logprob(old_config)\n",
    "                      - new_old_logprob)\n",
    "    return torch.exp(logacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def metropolis(trialfunc, proposal,num_steps=100):\n",
    "#     config = torch.zeros(1)\n",
    "#     all_configs = []\n",
    "#     for step in range(num_steps):\n",
    "#         next_config = proposal(config)\n",
    "#         with torch.no_grad():\n",
    "#             acc = mh_acceptance_ratio(trialfunc, config, next_config)\n",
    "#         rand_val = np.random.rand()\n",
    "#         if rand_val <= acc:\n",
    "#             config = next_config\n",
    "#         all_configs.append(config.clone()) # can we skip clone here?\n",
    "#     return torch.stack(all_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_symmetric(trialfunc, proposal, num_walkers=2,num_steps=100):\n",
    "    # with more walkers\n",
    "    # design choice: walkers are always the batch dim\n",
    "    config = torch.zeros(num_walkers, 1)\n",
    "    all_configs = []\n",
    "    for step in range(num_steps):\n",
    "        next_config = proposal(config)\n",
    "        with torch.no_grad():\n",
    "            acc = symmetric_mh_acceptance_ratio(trialfunc, config, next_config)\n",
    "        # acc shape should be (num_walkers, 1)\n",
    "            accept_or_reject = Bernoulli(acc).sample() # accept is 1, reject is 0\n",
    "            config = accept_or_reject*next_config + (1.0 - accept_or_reject)*config\n",
    "            all_configs.append(config.clone()) # can we skip clone here?\n",
    "    return torch.stack(all_configs, dim=1) # dim=1 to make walkers be the batch dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_asymmetric(trialfunc, proposal, num_walkers=2,num_steps=100):\n",
    "    # with more walkers\n",
    "    # design choice: walkers are always the batch dim\n",
    "    config = torch.zeros(num_walkers, 1)\n",
    "    all_configs = []\n",
    "    for step in range(num_steps):\n",
    "        next_config, next_current_logprob, current_next_logprob = proposal(config)\n",
    "        with torch.no_grad():\n",
    "            acc = asymmetric_mh_acceptance_ratio(trialfunc, \n",
    "                                                 next_current_logprob, \n",
    "                                                 current_next_logprob,\n",
    "                                                 config, next_config)\n",
    "            # acc shape should be (num_walkers, 1)\n",
    "            accept_or_reject = Bernoulli(acc).sample() # accept is 1, reject is 0\n",
    "            config = accept_or_reject*next_config + (1.0 - accept_or_reject)*config\n",
    "            all_configs.append(config.clone()) # can we skip clone here?\n",
    "    return torch.stack(all_configs, dim=1) # dim=1 to make walkers be the batch dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unadjusted_langevin(trialfunc, num_walkers=2, num_steps=100, eta=0.01):\n",
    "    # seems hard to get this to converge\n",
    "    config = torch.zeros(num_walkers, 1, requires_grad=True)\n",
    "    grad_out = torch.ones_like(config, requires_grad=False)\n",
    "    brownian_dist = Normal(torch.zeros_like(config, requires_grad=False),\n",
    "                          torch.ones_like(config, requires_grad=False))\n",
    "    all_configs = []\n",
    "    for step in range(num_steps):\n",
    "        # next config is from Langevin proposal (grad of logprob + gaussian noise)\n",
    "        curr_config_logprobs = trialfunc(config)\n",
    "        grads, = torch.autograd.grad(curr_config_logprobs,\n",
    "                                    config,\n",
    "                                    grad_outputs=grad_out,\n",
    "                                    retain_graph=False)\n",
    "        with torch.no_grad():\n",
    "            next_config = config + eta*grads + np.sqrt(2.0*eta)*brownian_dist.sample()\n",
    "        # then just append\n",
    "        next_config.requires_grad_(True)\n",
    "        all_configs.append(next_config)\n",
    "        config = next_config\n",
    "    return torch.stack(all_configs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unadjusted_langevin(trialfunc, num_walkers=2, num_steps=100, eta=0.01):\n",
    "    # seems hard to get this to converge\n",
    "    config = torch.zeros(num_walkers, 1, requires_grad=True)\n",
    "    grad_out = torch.ones_like(config, requires_grad=False)\n",
    "    all_configs = []\n",
    "    for step in range(num_steps):\n",
    "        # next config is from Langevin proposal (grad of logprob + gaussian noise)\n",
    "        curr_config_logprobs = trialfunc(config)\n",
    "        grads, = torch.autograd.grad(curr_config_logprobs,\n",
    "                                    config,\n",
    "                                    grad_outputs=grad_out,\n",
    "                                    retain_graph=False)\n",
    "        with torch.no_grad():\n",
    "            propdist = Normal(config + eta*grads, np.sqrt(2.0*eta))\n",
    "            next_config = propdist.sample()\n",
    "        # then just append\n",
    "        next_config.requires_grad_(True)\n",
    "        all_configs.append(next_config)\n",
    "        config = next_config\n",
    "    return torch.stack(all_configs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mala(trialfunc, num_walkers=2, num_steps=100, eta=0.01):\n",
    "    # this isn't right -- we need different MH filter for asymmetric proposal\n",
    "    config = torch.zeros(num_walkers, 1)\n",
    "    grad_out = torch.ones_like(config, requires_grad=False)\n",
    "    brownian_dist = Normal(torch.zeros_like(config, requires_grad=False),\n",
    "                          torch.ones_like(config, requires_grad=False))\n",
    "\n",
    "\n",
    "    def mala_proposal(old_point):\n",
    "        # \n",
    "        old_point.requires_grad_(True)\n",
    "        curr_config_logprobs = trialfunc(old_point)\n",
    "        grads, = torch.autograd.grad(curr_config_logprobs,\n",
    "                                    old_point,\n",
    "                                    grad_outputs=grad_out,\n",
    "                                    retain_graph=False)\n",
    "        with torch.no_grad():\n",
    "            propdist = Normal(old_point + eta*grads, np.sqrt(2.0*eta))\n",
    "            next_config = propdist.sample()\n",
    "            new_old_logprob = propdist.log_prob(next_config)\n",
    "        next_config.requires_grad_(True)\n",
    "        next_config_logprobs = trialfunc(next_config)\n",
    "        next_grads, = torch.autograd.grad(next_config_logprobs,\n",
    "                                    next_config,\n",
    "                                    grad_outputs=grad_out,\n",
    "                                    retain_graph=False)\n",
    "        with torch.no_grad():\n",
    "            reverse_propdist = Normal(next_config + eta*next_grads, np.sqrt(2.0*eta))\n",
    "            old_new_logprob = reverse_propdist.log_prob(old_point)\n",
    "        return next_config, new_old_logprob, old_new_logprob\n",
    "    \n",
    "    return metropolis_asymmetric(trialfunc, mala_proposal, num_walkers=num_walkers, num_steps=num_steps)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_minimize_step(trialfunc, samples, optimizer):\n",
    "    with torch.no_grad():\n",
    "        local_energies = trialfunc.local_energy(samples)\n",
    "    sample_logprobs = trialfunc(samples)\n",
    "    loss = (local_energies * sample_logprobs).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla \\mathbb{E}[f(x)] = \\mathbb{E}[f(x) \\nabla \\log \\pi(x)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = HarmonicTrialFunction(torch.tensor(1.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = metropolis_symmetric(tf, normal_proposal, num_walkers=100, num_steps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.067222222222222"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_mean_energy(1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0666, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(tf.local_energy(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = mala(tf, num_walkers=100, num_steps=20000, eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0649, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(tf.local_energy(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vmc_iterate(tf, num_iters=100):\n",
    "    opt = optim.SGD(tf.parameters(), lr=1e-2,momentum=0.9)\n",
    "    for i in range(num_iters):\n",
    "        results=metropolis_symmetric(tf, normal_proposal, num_walkers=1000, num_steps=5000)\n",
    "        energy_minimize_step(tf, results, opt)\n",
    "        print(tf.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(1.1938, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.1822, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.1659, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.1460, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.1232, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0986, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0729, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0473, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0224, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9991, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9782, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9601, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9455, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9348, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9280, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9251, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9259, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9300, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9368, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9458, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9562, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9675, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9789, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9900, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0005, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0098, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0178, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0243, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0292, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0324, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0341, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0343, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0332, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0308, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0275, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0235, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0189, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0140, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0091, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0043, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9999, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9959, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9925, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9896, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9875, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9862, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9855, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9855, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9861, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9872, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9887, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9905, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9926, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9947, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9969, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9990, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0009, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0025, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0039, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0050, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0058, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0063, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0064, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0063, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0059, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0053, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0045, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0036, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0026, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0016, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0005, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9996, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9987, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9980, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9974, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9970, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9968, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9966, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9966, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9967, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9969, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9972, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9976, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9980, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9985, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9991, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(0.9996, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0001, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0005, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0008, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0011, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0013, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0014, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0014, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0013, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0012, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0010, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0007, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0004, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.0001, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vmc_iterate(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
